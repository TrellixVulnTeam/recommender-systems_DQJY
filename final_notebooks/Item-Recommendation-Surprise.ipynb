{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import KFold, cross_validate\n",
    "from tabulate import tabulate\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Using Surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and load the data using Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML100K\n",
      "    userID  itemID  rating  timestamp\n",
      "0     196     242       3  881250949\n",
      "1     186     302       3  891717742\n",
      "2      22     377       1  878887116\n",
      "3     244      51       2  880606923\n",
      "4     166     346       1  886397596\n",
      "\n",
      "\n",
      "\n",
      "PDA2018\n",
      "    userID  itemID  rating  timeStamp\n",
      "0       5     648       5  978297876\n",
      "1       5    1394       5  978298237\n",
      "2       5    3534       5  978297149\n",
      "3       5     104       4  978298558\n",
      "4       5    2735       5  978297919\n",
      "Shapes before sub-sampling:\n",
      "(100000, 4)\n",
      "(470711, 4)\n",
      "\n",
      "Shapes after sub-sampling:\n",
      "(97623, 4)\n",
      "(465154, 4)\n",
      "\n",
      " Some general information on the training sets we will be using:\n",
      "1) Number of items in each dataset  ML100k: 1682 PDA: 1824\n",
      "2) Number of users in each dataset  ML100k: 943 PDA: 5690\n",
      "3) Number of ratings in each dataset  ML100k: 100000 PDA: 470711\n",
      "4) Mean rating  ML100k: 3.52986 PDA: 3.638361967321775\n"
     ]
    }
   ],
   "source": [
    "# Read both the datasets from the files using pandas\n",
    "movielens_df = pd.read_csv(\"../data/u.data\", sep=\"\\t\", header=None)\n",
    "movielens_df.columns = [\"userID\", \"itemID\", \"rating\", \"timestamp\"]\n",
    "pda_df = pd.read_csv(\"../data/train-PDA2018.csv\", sep=\",\")\n",
    "print(\"ML100K\\n\", movielens_df.head())\n",
    "print(\"\\n\\n\")\n",
    "print(\"PDA2018\\n\", pda_df.head())\n",
    "\n",
    "# Sample the data such that every user has rated at least 10 items and every item has been by at least 10 users\n",
    "print(\"Shapes before sub-sampling:\")\n",
    "print(movielens_df.shape)\n",
    "print(pda_df.shape)\n",
    "\n",
    "# Movielens users all have at least 20 ratings so no need to subsample the user values\n",
    "ml_subsampled = movielens_df[movielens_df['itemID'].isin(movielens_df['itemID'].value_counts()[movielens_df['itemID'].value_counts()>10].index)]\n",
    "pda_subsampled = pda_df[pda_df['itemID'].isin(pda_df['itemID'].value_counts()[pda_df['itemID'].value_counts()>10].index)]\n",
    "pda_subsampled = pda_subsampled[pda_subsampled['userID'].isin(pda_subsampled['userID'].value_counts()[pda_subsampled['userID'].value_counts()>10].index)]\n",
    "print(\"\\nShapes after sub-sampling:\")\n",
    "print(ml_subsampled.shape)\n",
    "print(pda_subsampled.shape)\n",
    "\n",
    "# Create the training datasets using Surprise's reader class\n",
    "reader = Reader(rating_scale=(1,5)) # We have ratings from 1 to 5 so we create the rating scale\n",
    "\n",
    "# Load the data from the dataframes\n",
    "movielens_dataset = Dataset.load_from_df(movielens_df.iloc[:,0:3], reader)\n",
    "pda_dataset = Dataset.load_from_df(pda_df.iloc[:,0:3], reader)\n",
    "\n",
    "# Build full trainsets to print out the data loaded above\n",
    "mls_train = movielens_dataset.build_full_trainset()\n",
    "pda_train = pda_dataset.build_full_trainset()\n",
    "\n",
    "# Print out some basic information about the datasets\n",
    "print(\"\\n Some general information on the training sets we will be using:\")\n",
    "print(\"1) Number of items in each dataset\", \" ML100k:\", mls_train.n_items, \"PDA:\", pda_train.n_items)\n",
    "print(\"2) Number of users in each dataset\", \" ML100k:\", mls_train.n_users, \"PDA:\", pda_train.n_users)\n",
    "print(\"3) Number of ratings in each dataset\", \" ML100k:\", mls_train.n_ratings, \"PDA:\", pda_train.n_ratings)\n",
    "print(\"4) Mean rating\", \" ML100k:\", mls_train.global_mean, \"PDA:\", pda_train.global_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surprise experiment parameters and variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Define auxilliary functions to get Top-N and then calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have ratings in the train set for all the users in the test set\n",
    "def find_items_not_in_trainset(trainset, testset):\n",
    "    items_not_in_train = []\n",
    "    for _,itemId, _ in testset:    \n",
    "        if itemId not in trainset.ir.keys():\n",
    "            items_not_in_train.append(itemId)\n",
    "            \n",
    "    return items_not_in_train\n",
    "\n",
    "        \n",
    "def user_seen_items(userId):\n",
    "    return [train_itemId for train_itemId, rating in mls_train.ur[userId]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTopN(predictions, n, minimumRating, criterion):\n",
    "    topN = defaultdict(list)\n",
    "    \n",
    "    for index, row in predictions.iterrows():\n",
    "        if (row[criterion] >= minimumRating):\n",
    "            topN[int(row.uid)].append((int(row.iid), row[criterion]))\n",
    "\n",
    "    for userID, ratings in topN.items():\n",
    "        ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        topN[int(userID)] = ratings[:n]\n",
    "\n",
    "    return topN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, items_not_in_train, k, threshold=3.5):\n",
    "    top_n_recoms_est = GetTopN(predictions, n=k, minimumRating=threshold, criterion=\"est\")\n",
    "    top_n_recoms_real = GetTopN(predictions, n=k, minimumRating=threshold, criterion=\"r_ui\")\n",
    "    above_threshold = predictions[predictions.r_ui >= threshold]\n",
    "\n",
    "    precisions = {}\n",
    "    recalls = {}\n",
    "\n",
    "    for uid, est_topn in top_n_recoms_est.items():\n",
    "        # Get items the user has already rated\n",
    "        already_seen = user_seen_items(uid)\n",
    "        # Get relevant items for the user\n",
    "        n_rel_for_user = len(above_threshold[above_threshold.uid == uid])        \n",
    "        tp = 0\n",
    "        # Penalize the scores if:\n",
    "        # - The item we are recommending was never seen in the training set (how could we recommend what we don't know?)\n",
    "        # - The user has already rated this item: It's not a good recommendation since the user already knows it/has seen it\n",
    "        for est_itemId, _ in est_topn:\n",
    "            if(est_itemId in items_not_in_train or est_itemId in already_seen):\n",
    "                tp += 0\n",
    "            else:\n",
    "                for real_itemId, _ in top_n_recoms_real[uid]:\n",
    "                    if (est_itemId == real_itemId):\n",
    "                        tp +=1\n",
    "        \n",
    "        precisions[uid] = tp/k\n",
    "        recalls[uid] = tp/n_rel_for_user if n_rel_for_user != 0 else 0\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "def ndcg_for_rec(predictions, threshold=3.5):\n",
    "    top_n_recoms_est = GetTopN(predictions, n=predictions.shape[0], minimumRating=threshold, criterion=\"est\")\n",
    "    top_n_recoms_real = GetTopN(predictions, n=predictions.shape[0], minimumRating=threshold, criterion=\"r_ui\")\n",
    "    ndcgs = {}\n",
    "\n",
    "    for uid, real_topn in top_n_recoms_real.items():\n",
    "        user_gt = [true_rating[0] for true_rating in real_topn]\n",
    "        est_top_for_current = top_n_recoms_est[uid]\n",
    "        predicted_items = [est_rating[0] for est_rating in est_top_for_current]\n",
    "        predicted_scores = [est_rating[1] for est_rating in est_top_for_current]\n",
    "        gain_scores = np.zeros(len(user_gt)).tolist()\n",
    "\n",
    "        ndcg_val = 0\n",
    "        \n",
    "        if(len(predicted_items) == 0):\n",
    "            ndcg_val += 0\n",
    "        elif(len(predicted_items) == 1 or len(user_gt) == 1):\n",
    "            if(predicted_items[0] == user_gt[0]):\n",
    "                ndcg_val += 1\n",
    "        else:\n",
    "            for i, pred_item in enumerate(predicted_items):\n",
    "                for j, gt_item in enumerate(user_gt):\n",
    "                    if(gt_item==pred_item):\n",
    "                        gain_scores[j] = i\n",
    "            gain_scores =[int(elem) for elem in gain_scores]\n",
    "            ndcg_val += ndcg_score(np.asarray([user_gt]), np.asarray([gain_scores]))\n",
    "            \n",
    "        #print(\"GT:\", user_gt)\n",
    "        #print(\"Pred:\", predicted_items)\n",
    "        #print(gain_scores)\n",
    "        #print( \"\\n\\n\")\n",
    "        ndcgs[uid] = ndcg_val\n",
    "\n",
    "    return ndcgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" EXPERIMENT VARIABLES\"\"\"\n",
    "# List that will contain the results of all experiments\n",
    "results_table = []\n",
    "kf = KFold(n_splits=5) # define number of k splits for cross validation using Surprise KFold\n",
    "\n",
    "# Algorithms we will be using in this section\n",
    "algorithms = {\n",
    "    \"UserKNN\": KNNWithMeans(k=60, sim_options={'name': 'cosine', 'shrinkage': 25, 'user_based': True}),\n",
    "    \"ItemKNN\": KNNWithMeans(k=40, sim_options={'name': 'cosine', 'shrinkage': 25, 'user_based': False}),\n",
    "}\n",
    "# Datasets\n",
    "datasets = {\n",
    "    \"ML100\": movielens_dataset,\n",
    "    \"PDA2018\": pda_dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5-fold cross validation with UserKNN on ML100 dataset ...\n",
      "Fold  number 0\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "12\n",
      "\n",
      "Running 5-fold cross validation with ItemKNN on ML100 dataset ...\n",
      "Fold  number 0\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "6\n",
      "\n",
      "Running 5-fold cross validation with UserKNN on PDA2018 dataset ...\n",
      "Fold  number 0\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "13\n",
      "\n",
      "Running 5-fold cross validation with ItemKNN on PDA2018 dataset ...\n",
      "Fold  number 0\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets.keys():\n",
    "    for algorithm in algorithms.keys():\n",
    "        print(\"Running 5-fold cross validation with\", algorithm, \"on\", dataset, \"dataset ...\")\n",
    "        \n",
    "        # Run cross validation\n",
    "        best_prec_5 = 0\n",
    "        best_prec_10 = 0\n",
    "        best_rec_5 = 0\n",
    "        best_rec_10 = 0\n",
    "        fold_nr = 0\n",
    "        \n",
    "        for trainset, testset in kf.split(movielens_dataset):\n",
    "            print(\"Fold  number\", fold_nr)\n",
    "            algorithms[algorithm].fit(trainset)\n",
    "            predictions = algorithms[algorithm].test(testset)\n",
    "            predictions_df = pd.DataFrame(predictions)\n",
    "            predictions_df = predictions_df.iloc[:,:-1]\n",
    "            items_not_in_trainset = find_items_not_in_trainset(trainset, testset)\n",
    "            print(len(items_not_in_trainset))\n",
    "            pres_at_5, recalls_at_5 = precision_recall_at_k(predictions_df, items_not_in_trainset, k=5)\n",
    "            pres_at_10, recalls_at_10 = precision_recall_at_k(predictions_df, items_not_in_trainset, k=10)\n",
    "            ndcgs = ndcg_for_rec(predictions_df)\n",
    "            avg_pre_5 = np.array([pres_at_5[k] for k in pres_at_5.keys()]).mean()\n",
    "            avg_rec_5 = np.array([recalls_at_5[k] for k in recalls_at_5.keys()]).mean()\n",
    "            avg_pre_10 = np.array([pres_at_10[k] for k in pres_at_10.keys()]).mean()\n",
    "            avg_rec_10 = np.array([recalls_at_10[k] for k in recalls_at_10.keys()]).mean()\n",
    "            avg_ndcg = np.array([ndcgs[k] for k in ndcgs.keys()]).mean()\n",
    "            if (avg_pre_5 > best_prec_5):\n",
    "                best_prec_5 = avg_pre_5\n",
    "            if (avg_pre_10 > best_prec_10):\n",
    "                best_prec_10 = avg_pre_10\n",
    "            if (avg_rec_5 > best_rec_5):\n",
    "                best_rec_5 = avg_rec_5\n",
    "            if (avg_rec_10 > best_rec_10):\n",
    "                best_rec_10 = avg_rec_10\n",
    "            fold_nr += 1\n",
    "            break\n",
    "        \n",
    "        new_line = [dataset+\"-\"+algorithm, best_prec_5, best_prec_10, best_rec_5, best_rec_10, avg_ndcg]\n",
    "        results_table.append(new_line)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Recommender     |    Pre@5 |   Pre@10 |    Rec@5 |   Rec@10 |     NDCG |\n",
      "|:----------------|---------:|---------:|---------:|---------:|---------:|\n",
      "| ML100-UserKNN   | 0.353387 | 0.360161 | 0.320818 | 0.477432 | 0.719624 |\n",
      "| ML100-ItemKNN   | 0.345903 | 0.348597 | 0.306494 | 0.452833 | 0.733329 |\n",
      "| PDA2018-UserKNN | 0.345392 | 0.358295 | 0.302015 | 0.459918 | 0.729926 |\n",
      "| PDA2018-ItemKNN | 0.352242 | 0.354596 | 0.315589 | 0.458352 | 0.732392 |\n"
     ]
    }
   ],
   "source": [
    "# Display results of running the algorithms\n",
    "results_table_headers = [\"Recommender\", \"Pre@5\", \"Pre@10\", \"Rec@5\", \"Rec@10\", \"NDCG\"]\n",
    "print(tabulate(results_table, results_table_headers, tablefmt=\"pipe\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "# Export the results to a csv file\n",
    "# results_df = pd.DataFrame(results_table, columns=[\"Recommender\", \"Pre@5\", \"Pre@10\", \"Rec@5\", \"Rec@10\", \"NDCG\"])\n",
    "# results_df.to_csv(\"../data/items_recommendation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
