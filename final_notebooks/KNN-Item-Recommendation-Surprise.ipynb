{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import KNNWithMeans, KNNBasic\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import KFold, cross_validate\n",
    "from tabulate import tabulate\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Using Surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and load the data using Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML100K\n",
      "    userID  itemID  rating  timestamp\n",
      "0     196     242       3  881250949\n",
      "1     186     302       3  891717742\n",
      "2      22     377       1  878887116\n",
      "3     244      51       2  880606923\n",
      "4     166     346       1  886397596\n",
      "\n",
      "\n",
      "\n",
      "PDA2018\n",
      "    userID  itemID  rating  timeStamp\n",
      "0       5     648       5  978297876\n",
      "1       5    1394       5  978298237\n",
      "2       5    3534       5  978297149\n",
      "3       5     104       4  978298558\n",
      "4       5    2735       5  978297919\n",
      "Shapes before sub-sampling:\n",
      "(100000, 4)\n",
      "(470711, 4)\n",
      "\n",
      "Shapes after sub-sampling:\n",
      "(97623, 4)\n",
      "(465154, 4)\n",
      "\n",
      " Some general information on the training sets we will be using:\n",
      "1) Number of items in each dataset  ML100k: 1682 PDA: 1824\n",
      "2) Number of users in each dataset  ML100k: 943 PDA: 5690\n",
      "3) Number of ratings in each dataset  ML100k: 100000 PDA: 470711\n",
      "4) Mean rating  ML100k: 3.52986 PDA: 3.638361967321775\n"
     ]
    }
   ],
   "source": [
    "# Read both the datasets from the files using pandas\n",
    "movielens_df = pd.read_csv(\"../data/u.data\", sep=\"\\t\", header=None)\n",
    "movielens_df.columns = [\"userID\", \"itemID\", \"rating\", \"timestamp\"]\n",
    "pda_df = pd.read_csv(\"../data/train-PDA2018.csv\", sep=\",\")\n",
    "print(\"ML100K\\n\", movielens_df.head())\n",
    "print(\"\\n\\n\")\n",
    "print(\"PDA2018\\n\", pda_df.head())\n",
    "\n",
    "# Sample the data such that every user has rated at least 10 items and every item has been by at least 10 users\n",
    "print(\"Shapes before sub-sampling:\")\n",
    "print(movielens_df.shape)\n",
    "print(pda_df.shape)\n",
    "\n",
    "# Movielens users all have at least 20 ratings so no need to subsample the user values\n",
    "ml_subsampled = movielens_df[movielens_df['itemID'].isin(movielens_df['itemID'].value_counts()[movielens_df['itemID'].value_counts()>10].index)]\n",
    "pda_subsampled = pda_df[pda_df['itemID'].isin(pda_df['itemID'].value_counts()[pda_df['itemID'].value_counts()>10].index)]\n",
    "pda_subsampled = pda_subsampled[pda_subsampled['userID'].isin(pda_subsampled['userID'].value_counts()[pda_subsampled['userID'].value_counts()>10].index)]\n",
    "print(\"\\nShapes after sub-sampling:\")\n",
    "print(ml_subsampled.shape)\n",
    "print(pda_subsampled.shape)\n",
    "\n",
    "# Create the training datasets using Surprise's reader class\n",
    "reader = Reader(rating_scale=(1,5)) # We have ratings from 1 to 5 so we create the rating scale\n",
    "\n",
    "# Load the data from the dataframes\n",
    "movielens_dataset = Dataset.load_from_df(movielens_df.iloc[:,0:3], reader)\n",
    "pda_dataset = Dataset.load_from_df(pda_df.iloc[:,0:3], reader)\n",
    "\n",
    "# Build full trainsets to print out the data loaded above\n",
    "mls_train = movielens_dataset.build_full_trainset()\n",
    "pda_train = pda_dataset.build_full_trainset()\n",
    "\n",
    "# Print out some basic information about the datasets\n",
    "print(\"\\n Some general information on the training sets we will be using:\")\n",
    "print(\"1) Number of items in each dataset\", \" ML100k:\", mls_train.n_items, \"PDA:\", pda_train.n_items)\n",
    "print(\"2) Number of users in each dataset\", \" ML100k:\", mls_train.n_users, \"PDA:\", pda_train.n_users)\n",
    "print(\"3) Number of ratings in each dataset\", \" ML100k:\", mls_train.n_ratings, \"PDA:\", pda_train.n_ratings)\n",
    "print(\"4) Mean rating\", \" ML100k:\", mls_train.global_mean, \"PDA:\", pda_train.global_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surprise experiment parameters and variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define auxilliary functions to get Top-N and then calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have ratings in the train set for all the users in the test set\n",
    "def find_items_not_in_trainset(trainset, testset):\n",
    "    items_not_in_train = []\n",
    "    for _,itemId, _ in testset:    \n",
    "        if itemId not in trainset.ir.keys():\n",
    "            items_not_in_train.append(itemId)\n",
    "            \n",
    "    return items_not_in_train\n",
    "\n",
    "        \n",
    "def user_seen_items(userId):\n",
    "    return [train_itemId for train_itemId, rating in mls_train.ur[userId]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get top-n from a dataframe\n",
    "def get_top_n_df(predictions, n, minimumRating, criterion):\n",
    "    topN = defaultdict(list)\n",
    "    \n",
    "    for index, row in predictions.iterrows():\n",
    "        if (row[criterion] >= minimumRating):\n",
    "            topN[int(row.uid)].append((int(row.iid), row[criterion]))\n",
    "\n",
    "    for userID, ratings in topN.items():\n",
    "        ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        topN[int(userID)] = ratings[:n]\n",
    "\n",
    "    return topN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top-n from the crude surprise predictions object\n",
    "def get_top_n_dict(predictions, n, minimumRating):\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, items_not_in_train, k, threshold=3.5):\n",
    "    top_n_recoms_est = get_top_n_df(predictions, n=k, minimumRating=threshold, criterion=\"est\")\n",
    "    top_n_recoms_real = get_top_n_df(predictions, n=k, minimumRating=threshold, criterion=\"r_ui\")\n",
    "    above_threshold = predictions[predictions.r_ui >= threshold]\n",
    "\n",
    "    precisions = {}\n",
    "    recalls = {}\n",
    "\n",
    "    for uid, est_topn in top_n_recoms_est.items():\n",
    "        # Get items the user has already rated\n",
    "        already_seen = user_seen_items(uid)\n",
    "        # Get relevant items for the user\n",
    "        n_rel_for_user = len(above_threshold[above_threshold.uid == uid])        \n",
    "        tp = 0\n",
    "        # Penalize the scores if:\n",
    "        # - The item we are recommending was never seen in the training set (how could we recommend what we don't know?)\n",
    "        # - The user has already rated this item: It's not a good recommendation since the user already knows it/has seen it\n",
    "        for est_itemId, _ in est_topn:\n",
    "            if(est_itemId in items_not_in_train or est_itemId in already_seen):\n",
    "                tp += 0\n",
    "            else:\n",
    "                for real_itemId, _ in top_n_recoms_real[uid]:\n",
    "                    if (est_itemId == real_itemId):\n",
    "                        tp +=1\n",
    "        \n",
    "        precisions[uid] = tp/k\n",
    "        recalls[uid] = tp/n_rel_for_user if n_rel_for_user != 0 else 0\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_for_rec(predictions, items_not_in_train, k, threshold=3.5):\n",
    "    top_n_recoms_est = get_top_n_dict(predictions, n=k, minimumRating=threshold)\n",
    "    \n",
    "    user_ratings = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        user_ratings[uid].append((iid,true_r,est))    \n",
    "        \n",
    "    ndcgs = defaultdict()\n",
    "    \n",
    "    # First compute the idealized DCG, meaning that the ranking is perfect\n",
    "    idcg = 1 + (sum([1/np.log2(i) for i in range(2, k+1)]))\n",
    "    \n",
    "    for user, ratings in user_ratings.items():    \n",
    "        already_seen = user_seen_items(uid)\n",
    "\n",
    "        rel_for_user = [iid for iid, true_r, est in ratings if true_r >= threshold]\n",
    "        \n",
    "        if len(rel_for_user) == 0:\n",
    "            ndcgs[user] = 0\n",
    "            continue # if there no relevant items for this user then proceed to the next one\n",
    "        \n",
    "        dcg = []\n",
    "        for idx, item in enumerate(top_n_recoms_est[user]):\n",
    "            iid, est = item[0], item[1]\n",
    "            if(iid in items_not_in_train or iid in already_seen):\n",
    "                dcg.append(0)\n",
    "            else:    \n",
    "                if iid in rel_for_user and est >= threshold:\n",
    "                    if idx == 0:\n",
    "                        dcg.append(1)\n",
    "                    else:\n",
    "                        dcg.append(1/np.log2(idx+1))\n",
    "\n",
    "        ndcgs[user] = sum(dcg)/idcg\n",
    "\n",
    "    return ndcgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" EXPERIMENT VARIABLES\"\"\"\n",
    "# List that will contain the results of all experiments\n",
    "results_table = []\n",
    "kf = KFold(n_splits=5) # define number of k splits for cross validation using Surprise KFold\n",
    "\n",
    "# Algorithms we will be using in this section\n",
    "algorithms = {\n",
    "    \"UserKNN\": KNNBasic(k=80, sim_options={'name': 'cosine', 'user_based': True}),\n",
    "    \"ItemKNN\": KNNBasic(k=80, sim_options={'name': 'cosine', 'user_based': False}),\n",
    "}\n",
    "# Datasets\n",
    "datasets = {\n",
    "    \"ML100\": movielens_dataset,\n",
    "    \"PDA2018\": pda_dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 5-fold cross validation with UserKNN on ML100 dataset ...\n",
      "Fold  number 1\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 2\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 3\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 4\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 5\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\n",
      "Running 5-fold cross validation with ItemKNN on ML100 dataset ...\n",
      "Fold  number 1\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 2\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 3\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 4\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 5\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\n",
      "Running 5-fold cross validation with UserKNN on PDA2018 dataset ...\n",
      "Fold  number 1\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 2\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 3\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 4\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 5\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\n",
      "Running 5-fold cross validation with ItemKNN on PDA2018 dataset ...\n",
      "Fold  number 1\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 2\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 3\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 4\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Fold  number 5\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets.keys():\n",
    "    for algorithm in algorithms.keys():\n",
    "        print(\"Running 5-fold cross validation with\", algorithm, \"on\", dataset, \"dataset ...\")\n",
    "        \n",
    "        # Run cross validation\n",
    "        best_prec_5 = 0\n",
    "        best_prec_10 = 0\n",
    "        best_rec_5 = 0\n",
    "        best_rec_10 = 0\n",
    "        best_ndcg_5 = 0\n",
    "        best_ndcg_10 = 0\n",
    "        fold_nr = 0\n",
    "        \n",
    "        for trainset, testset in kf.split(movielens_dataset):\n",
    "            print(\"Fold  number\", fold_nr+1)\n",
    "            algorithms[algorithm].fit(trainset)\n",
    "            predictions = algorithms[algorithm].test(testset)            \n",
    "            predictions_df = pd.DataFrame(predictions)\n",
    "            predictions_df = predictions_df.iloc[:,:-1]\n",
    "            items_not_in_trainset = find_items_not_in_trainset(trainset, testset)\n",
    "            pres_at_5, recalls_at_5 = precision_recall_at_k(predictions_df, items_not_in_trainset, k=5)\n",
    "            pres_at_10, recalls_at_10 = precision_recall_at_k(predictions_df, items_not_in_trainset, k=10)\n",
    "            ndcgs_5 = ndcg_for_rec(predictions, items_not_in_trainset, 5)\n",
    "            ndcgs_10 = ndcg_for_rec(predictions, items_not_in_trainset, 10)\n",
    "            avg_pre_5 = np.array([pres_at_5[k] for k in pres_at_5.keys()]).mean()\n",
    "            avg_rec_5 = np.array([recalls_at_5[k] for k in recalls_at_5.keys()]).mean()\n",
    "            avg_pre_10 = np.array([pres_at_10[k] for k in pres_at_10.keys()]).mean()\n",
    "            avg_rec_10 = np.array([recalls_at_10[k] for k in recalls_at_10.keys()]).mean()\n",
    "            avg_ndcg_5 = np.array([ndcgs_5[k] for k in ndcgs_5.keys()]).mean()\n",
    "            avg_ndcg_10 = np.array([ndcgs_10[k] for k in ndcgs_10.keys()]).mean()\n",
    "            if (avg_pre_5 > best_prec_5):\n",
    "                best_prec_5 = avg_pre_5\n",
    "            if (avg_pre_10 > best_prec_10):\n",
    "                best_prec_10 = avg_pre_10\n",
    "            if (avg_rec_5 > best_rec_5):\n",
    "                best_rec_5 = avg_rec_5\n",
    "            if (avg_rec_10 > best_rec_10):\n",
    "                best_rec_10 = avg_rec_10\n",
    "            if (avg_ndcg_5 > best_ndcg_5):\n",
    "                best_ndcg_5 = avg_ndcg_5\n",
    "            if (avg_ndcg_10 > best_ndcg_10):\n",
    "                best_ndcg_10 = avg_ndcg_5\n",
    "            fold_nr += 1\n",
    "        \n",
    "        \n",
    "        new_line = [dataset+\"-\"+algorithm, best_prec_5, best_prec_10, best_rec_5, best_rec_10, best_ndcg_5, best_ndcg_10]\n",
    "        results_table.append(new_line)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Recommender     |    Pre@5 |   Pre@10 |    Rec@5 |   Rec@10 |   NDCG@5 |   NDCG@10 |\n",
      "|:----------------|---------:|---------:|---------:|---------:|---------:|----------:|\n",
      "| ML100-UserKNN   | 0.353187 | 0.34989  | 0.316283 | 0.463249 | 0.678307 |  0.640064 |\n",
      "| ML100-ItemKNN   | 0.341703 | 0.385859 | 0.322389 | 0.515305 | 0.501535 |  0.481618 |\n",
      "| PDA2018-UserKNN | 0.345692 | 0.346674 | 0.314428 | 0.463595 | 0.659795 |  0.659795 |\n",
      "| PDA2018-ItemKNN | 0.336127 | 0.379209 | 0.312074 | 0.505894 | 0.501431 |  0.490198 |\n"
     ]
    }
   ],
   "source": [
    "# Display results of running the algorithms\n",
    "results_table_headers = [\"Recommender\", \"Pre@5\", \"Pre@10\", \"Rec@5\", \"Rec@10\", \"NDCG@5\", \"NDCG@10\"]\n",
    "print(tabulate(results_table, results_table_headers, tablefmt=\"pipe\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the results to a csv file\n",
    "results_df = pd.DataFrame(results_table, columns=[\"Recommender\", \"Pre@5\", \"Pre@10\", \"Rec@5\", \"Rec@10\", \"NDCG@5\", \"NDCG@10\"])\n",
    "results_df.to_csv(\"../data/knn_recommendation_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
