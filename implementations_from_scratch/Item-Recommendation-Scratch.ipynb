{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Movielens 100K data into train and test (80-20)\n",
    "np.seed = 1\n",
    "dataset = pd.read_csv(\"../data/u.data\",sep='\\t',names=\"user_id,item_id,rating,timestamp\".split(\",\"))\n",
    "dataset = dataset.iloc[:,:3]\n",
    "dataset.user_id = dataset.user_id.astype('category').cat.codes.values\n",
    "dataset.item_id = dataset.item_id.astype('category').cat.codes.values\n",
    "train, test = train_test_split(dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have ratings in the train set for all the users in the test set\n",
    "for test_user in test[\"user_id\"].values:\n",
    "    if test_user not in train[\"user_id\"].values:\n",
    "        print(\"User\", test_user, \"is in the test set but not in the train set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MostPop Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the items with the most interactions (ratings)\n",
    "item_counts = train.item_id.value_counts()\n",
    "\n",
    "# define mostpop algorithm\n",
    "def mostpop(data, k=5):\n",
    "    mostpop_items = item_counts.index[:k].values.tolist()\n",
    "    mostpop_topn = {}\n",
    "    for uid in data.user_id.values:\n",
    "        mostpop_topn[uid] = mostpop_items\n",
    "    return mostpop_topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 1682)\n",
      "(1682, 943)\n"
     ]
    }
   ],
   "source": [
    "# Compute similarities\n",
    "\n",
    "# First create the user-item matrix\n",
    "unique_users = dataset.user_id.unique()\n",
    "unique_items = dataset.item_id.unique()\n",
    "ui_matrix = np.zeros((unique_users.shape[0], unique_items.shape[0]))\n",
    "\n",
    "# Use train data to build the similarity matrix\n",
    "for train_row in train.itertuples():\n",
    "    ui_matrix[train_row.user_id - 1, train_row.item_id - 1] = train_row.rating\n",
    "\n",
    "print(ui_matrix.shape) # We should have a 943x1682 matrix\n",
    "\n",
    "# Item-User matrix is the transpose of the User-Item matrix\n",
    "iu_matrix = ui_matrix.T\n",
    "print(iu_matrix.shape) # We should have a 1682x943 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all construct the dictionary containing the ground truth rankings\n",
    "test_gt_ranks = {}\n",
    "for uid in test.user_id.unique():\n",
    "    user_items = test[test.user_id == uid]\n",
    "    user_items = user_items[user_items.rating > 3.5]\n",
    "    user_items = user_items.sort_values(by=[\"rating\"], ascending=False)\n",
    "    test_gt_ranks[uid] = user_items.item_id.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have ratings in the train set for all the users in the test set\n",
    "def find_items_not_in_trainset(trainset, testset):\n",
    "    items_not_in_train = []\n",
    "    for itemId in testset.item_id.values:    \n",
    "        if itemId not in trainset.item_id.values:\n",
    "            items_not_in_train.append(itemId)\n",
    "            \n",
    "    return items_not_in_train\n",
    "\n",
    "# This method just gets all the items the user has rated in the trainset        \n",
    "def user_seen_items(userId):\n",
    "    return train[train.user_id == userId].item_id.index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, items_not_in_train, k, threshold=3.5):\n",
    "    top_n_recoms_est = GetTopN(predictions, n=k, minimumRating=threshold, criterion=\"est\")\n",
    "    top_n_recoms_real = GetTopN(predictions, n=k, minimumRating=threshold, criterion=\"r_ui\")\n",
    "    above_threshold = predictions[predictions.r_ui >= threshold]\n",
    "\n",
    "    precisions = {}\n",
    "    recalls = {}\n",
    "\n",
    "    for uid, est_topn in top_n_recoms_est.items():\n",
    "        # Get items the user has already rated\n",
    "        already_seen = user_seen_items(uid)\n",
    "        # Get relevant items for the user\n",
    "        n_rel_for_user = len(above_threshold[above_threshold.uid == uid])        \n",
    "        tp = 0\n",
    "        # Penalize the scores if:\n",
    "        # - The item we are recommending was never seen in the training set (how could we recommend what we don't know?)\n",
    "        # - The user has already rated this item: It's not a good recommendation since the user already knows it/has seen it\n",
    "        for est_itemId, _ in est_topn:\n",
    "            if(est_itemId in items_not_in_train or est_itemId in already_seen):\n",
    "                tp += 0\n",
    "            else:\n",
    "                for real_itemId, _ in top_n_recoms_real[uid]:\n",
    "                    if (est_itemId == real_itemId):\n",
    "                        tp +=1\n",
    "        \n",
    "        precisions[uid] = tp/k\n",
    "        recalls[uid] = tp/n_rel_for_user if n_rel_for_user != 0 else 0\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Geri/.local/share/virtualenvs/recsys2020_project-EWQ0OJVi/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#def calc_precision_recall(predicted, k)\n",
    "#    total_precision = 0\n",
    "#    total_recall\n",
    "#\n",
    "#    for uid in test.user_id:\n",
    "#        gt_rank = test_gt_ranks[uid][:k]\n",
    "#        predicted = mostpop(uid, k)\n",
    "#        if (len(predicted) > len(gt_rank)):\n",
    "#            predicted = predicted[:len(gt_rank)]\n",
    "#\n",
    "#        total_precisions += precision_score(gt_rank, predicted, average='macro')\n",
    "#        total_recall += recall_score(gt_rank, predicted, average='macro')\n",
    "#    \n",
    "#    precisions = total_precisions/len(train)\n",
    "#    recall = total_recall/len(train)\n",
    "#\n",
    "#    return precisions, recall\n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008211845238095088"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prec_5, rec_5 = calc_precision_recall\n",
    "#print(\"Preci\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014964539007092197"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_precisions/940"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "precisions = {}\n",
    "recalls = {}\n",
    "predicted_topn = mostpop(test, k)\n",
    "\n",
    "for uid in predicted_topn.keys():\n",
    "    tp_prec = 0\n",
    "    tp_rec = 0\n",
    "    \n",
    "    gt_rank = test_gt_ranks[uid]\n",
    "    predicted_topn_rank = predicted_topn[uid]\n",
    "    \n",
    "    ## Precision for this user's top-n\n",
    "    for recom_item in predicted_topn_rank:\n",
    "        if recom_item in gt_rank[:5]:\n",
    "            tp_prec += 1\n",
    "    else:\n",
    "            tp_prec += 0\n",
    "    precisions[uid] = tp_prec/k\n",
    "\n",
    "    ## Recall for this user's top-n\n",
    "    for recom_item in predicted_topn_rank:\n",
    "        if recom_item in gt_rank:\n",
    "            tp_rec += 1\n",
    "    else:\n",
    "            tp_rec += 0\n",
    "    recalls[uid] = tp_rec/len(gt_rank) if len(gt_rank) != 0 else 0\n",
    "    \n",
    "    #if (len(predicted) > len(gt_rank)):\n",
    "    #    predicted = predicted[:len(gt_rank)]\n",
    "    #total_precisions += precision_score(gt_rank, predicted, average='weighted')\n",
    "    #total_recalls += recall_score(gt_rank, predicted, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.043266171792152704 0.049491027611216674\n"
     ]
    }
   ],
   "source": [
    "avg_pre = np.array([precisions[k] for k in precisions.keys()]).mean()\n",
    "avg_rec = np.array([recalls[k] for k in recalls.keys()]).mean()\n",
    "print(avg_pre, avg_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
