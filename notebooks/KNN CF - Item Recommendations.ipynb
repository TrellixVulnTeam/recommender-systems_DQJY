{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import KFold, cross_validate, train_test_split\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a KNN Ranking algorithm version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import AlgoBase\n",
    "from surprise import Dataset\n",
    "from six import iteritems\n",
    "import heapq\n",
    "from surprise.prediction_algorithms.predictions import PredictionImpossible\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class SymmetricAlgo(AlgoBase):\n",
    "    \"\"\"\n",
    "    This is an abstract class aimed to ease the use of symmetric algorithms.\n",
    "    A symmetric algorithm is an algorithm that can can be based on users or on\n",
    "    items indifferently. When the algo is user-based x denotes a user and y an item. \n",
    "    Else, it's reversed.\n",
    "    \"\"\"\n",
    "    def __init__(self, sim_options={}, verbose=True, **kwargs):\n",
    "\n",
    "        AlgoBase.__init__(self, sim_options=sim_options, **kwargs)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, trainset):\n",
    "\n",
    "        AlgoBase.fit(self, trainset)\n",
    "\n",
    "        ub = self.sim_options['user_based']\n",
    "        self.n_x = self.trainset.n_users if ub else self.trainset.n_items\n",
    "        self.n_y = self.trainset.n_items if ub else self.trainset.n_users\n",
    "        self.xr = self.trainset.ur if ub else self.trainset.ir\n",
    "        self.yr = self.trainset.ir if ub else self.trainset.ur\n",
    "\n",
    "        return self\n",
    "\n",
    "    def switch(self, u_stuff, i_stuff):\n",
    "        \"\"\"\n",
    "        Return x_stuff and y_stuff depending on the user_based field.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.sim_options['user_based']:\n",
    "            return u_stuff, i_stuff\n",
    "        else:\n",
    "            return i_stuff, u_stuff\n",
    "\n",
    "\n",
    "class RankingKNNBasic(SymmetricAlgo):\n",
    "    def __init__(self, k=40, min_k=1, sim_options={}, verbose=True, **kwargs):\n",
    "\n",
    "        SymmetricAlgo.__init__(self, sim_options=sim_options, verbose=verbose,\n",
    "                               **kwargs)\n",
    "        self.k = k\n",
    "        self.min_k = min_k\n",
    "\n",
    "    def fit(self, trainset):\n",
    "\n",
    "        SymmetricAlgo.fit(self, trainset)\n",
    "        self.sim = self.compute_similarities()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        # Penalize items which are unknown, don't just skip them fromt the computation\n",
    "        self.unknown_considerations = 0\n",
    "\n",
    "        if not (self.trainset.knows_user(u) or self.trainset.knows_item(i)):\n",
    "            self.unknown_considerations += 1\n",
    "            \n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible('User and/or item is unknown.')\n",
    "\n",
    "        x, y = self.switch(u, i)\n",
    "        \n",
    "        neighbors = [(self.sim[x, x2], r) for (x2, r) in self.yr[y]]\n",
    "        k_neighbors = heapq.nlargest(self.k, neighbors, key=lambda t: t[0])\n",
    "        \n",
    "        # compute only the sum of similarities (ranking)\n",
    "        sum_sim = actual_k = 0\n",
    "        sim_sums = []\n",
    "        for (sim, r) in k_neighbors:\n",
    "            sum_sim += sim\n",
    "            actual_k += 1\n",
    "            sim_sums.append(sum_sim)\n",
    "\n",
    "        if actual_k < self.min_k:\n",
    "            raise PredictionImpossible('Not enough neighbors.')\n",
    "        \n",
    "        est = sum_sim \n",
    "        \n",
    "        details = {'actual_k': actual_k, 'unknown_items': self.unknown_considerations}\n",
    "        return est, details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a way to get precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def precision_recall_at_k(predictions, trainset_user_ratings, k=5, threshold=3.5):\n",
    "    '''Return precision and recall at k metrics for each user.'''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    \n",
    "    for uid, iid, true_r, est, details in predictions:\n",
    "        user_est_true[uid].append((est, true_r, details, iid))\n",
    "    \n",
    "    precisions = dict()\n",
    "    recalls = dict()    \n",
    "    \n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        \n",
    "        # nr_uknowns \n",
    "        nr_unknowns = user_ratings[0][2]\n",
    "        nr_unknowns = nr_unknowns.get('unknown_items')\n",
    "        if nr_unknowns == None:\n",
    "            nr_unknowns = 0\n",
    "        \n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r, _ , _) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _ , _ , _) in user_ratings[:k])\n",
    "\n",
    "        # relevant and recommended items in top k\n",
    "        rel_and_rec_k = [(est, true_r, iid) for (est, true_r, _, iid) in user_ratings[:k] if ((true_r >= threshold) and (est >= threshold))]\n",
    "        \n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = len(rel_and_rec_k)\n",
    "        \n",
    "        #Â Number of recommended items which the user has already rated\n",
    "        # Penalize recommendations for items that are already in the train set for said user\n",
    "        for recommended in rel_and_rec_k:\n",
    "            for already_seen in trainset.ur[uid]:\n",
    "                if recommended[2] == already_seen[0]:\n",
    "                    n_rel_and_rec_k -= 1\n",
    "                    nr_unknowns += 1\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / (k + nr_unknowns)\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / (n_rel + nr_unknowns) if n_rel != 0 else 0\n",
    "\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userID  itemID  rating  timestamp\n",
      "0     196     242       3  881250949\n",
      "1     186     302       3  891717742\n",
      "2      22     377       1  878887116\n",
      "3     244      51       2  880606923\n",
      "4     166     346       1  886397596\n",
      "\n",
      "\n",
      "\n",
      "   userID  itemID  rating  timeStamp\n",
      "0       5     648       5  978297876\n",
      "1       5    1394       5  978298237\n",
      "2       5    3534       5  978297149\n",
      "3       5     104       4  978298558\n",
      "4       5    2735       5  978297919\n"
     ]
    }
   ],
   "source": [
    "# Read both the datasets from the files using pandas\n",
    "movielens_df = pd.read_csv(\"../data/u.data\", sep=\"\\t\", header=None)\n",
    "movielens_df.columns = [\"userID\", \"itemID\", \"rating\", \"timestamp\"]\n",
    "pda_df = pd.read_csv(\"../data/train-PDA2018.csv\", sep=\",\")\n",
    "print(movielens_df.head())\n",
    "print(\"\\n\\n\")\n",
    "print(pda_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General information on the training sets we will be using \n",
      "\n",
      "1) Number of items in each dataset  ML100k: 1646 PDA: 1822\n",
      "2) Number of users in each dataset  ML100k: 943 PDA: 5680\n",
      "3) Number of ratings in each dataset  ML100k: 80000 PDA: 376568\n",
      "4) Mean rating  ML100k: 3.5279875 PDA: 3.637459901000616\n"
     ]
    }
   ],
   "source": [
    "# Create the training datasets using Surprise's reader class\n",
    "reader = Reader(rating_scale=(1,5)) # We have ratings from 1 to 5 so we create the rating scale\n",
    "\n",
    "# Load the data from the dataframes\n",
    "movielens_dataset = Dataset.load_from_df(movielens_df.iloc[:,0:3], reader)\n",
    "pda_dataset = Dataset.load_from_df(pda_df.iloc[:,0:3], reader)\n",
    "\n",
    "# Build full trainsets to print out the data loaded above\n",
    "# mls_train = movielens_dataset.build_full_trainset()\n",
    "# pda_train = pda_dataset.build_full_trainset()\n",
    "mls_train, mls_test = train_test_split(data=movielens_dataset, test_size=0.2)\n",
    "pda_train, pda_test = train_test_split(data=pda_dataset, test_size=0.2)\n",
    "\n",
    "# Print out some basic information about the datasets\n",
    "print(\"General information on the training sets we will be using \\n\")\n",
    "print(\"1) Number of items in each dataset\", \" ML100k:\", mls_train.n_items, \"PDA:\", pda_train.n_items)\n",
    "print(\"2) Number of users in each dataset\", \" ML100k:\", mls_train.n_users, \"PDA:\", pda_train.n_users)\n",
    "print(\"3) Number of ratings in each dataset\", \" ML100k:\", mls_train.n_ratings, \"PDA:\", pda_train.n_ratings)\n",
    "print(\"4) Mean rating\", \" ML100k:\", mls_train.global_mean, \"PDA:\", pda_train.global_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold number 1\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "[Prediction(uid=200, iid=515, r_ui=5.0, est=5, details={'actual_k': 80, 'unknown_items': 0, 'was_impossible': False}), Prediction(uid=42, iid=627, r_ui=2.0, est=5, details={'actual_k': 40, 'unknown_items': 0, 'was_impossible': False})]\n",
      "Precision and Recall @5: 0.5357407968489623 0.18917065162084593\n",
      "Precision and Recall @10: 0.5222174079013951 0.35708878701143293\n",
      "\n",
      "Fold number 2\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "[Prediction(uid=276, iid=411, r_ui=4.0, est=5, details={'actual_k': 80, 'unknown_items': 0, 'was_impossible': False}), Prediction(uid=328, iid=127, r_ui=5.0, est=5, details={'actual_k': 80, 'unknown_items': 0, 'was_impossible': False})]\n",
      "Precision and Recall @5: 0.5416325809220824 0.18261461747527719\n",
      "Precision and Recall @10: 0.5328828470928153 0.35480548513953514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(2)\n",
    "fold_nr = 1\n",
    "user_algo = RankingKNNBasic(k=80, sim_options={'name': 'cosine', 'user_based': True})\n",
    "\n",
    "for trainset, testset in kf.split(movielens_dataset):\n",
    "    print(\"Fold number\", fold_nr)\n",
    "    user_algo.fit(trainset)\n",
    "    predictions = user_algo.test(testset)\n",
    "    print(predictions[:2])\n",
    "    pres_5, recs_5 = precision_recall_at_k(predictions, trainset.ur, k=5)\n",
    "    pres_10, recs_10 = precision_recall_at_k(predictions, trainset.ur, k=10)\n",
    "    \n",
    "    # Precision and recall averaged over all users for this fold\n",
    "    pre_5 = np.array([pres_5[k] for k in pres_5.keys()]).mean()\n",
    "    rec_5 = np.array([recs_5[k] for k in recs_5.keys()]).mean()\n",
    "    pre_10 = np.array([pres_10[k] for k in pres_10.keys()]).mean()\n",
    "    rec_10 = np.array([recs_10[k] for k in recs_10.keys()]).mean()\n",
    "\n",
    "    print(\"Precision and Recall @5:\", pre_5, rec_5)\n",
    "    print(\"Precision and Recall @10:\", pre_10, rec_10)\n",
    "    fold_nr += 1\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold number 1\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Precision and Recall @5: 0.5228298742614755 0.18602214898830088\n",
      "Precision and Recall @10: 0.5197578314227307 0.3568674805289799\n",
      "\n",
      "Fold number 2\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Precision and Recall @5: 0.5293457220286488 0.18673974609498895\n",
      "Precision and Recall @10: 0.5255943596876789 0.3574375039241169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fold_nr = 1\n",
    "item_algo = RankingKNNBasic(k=80, sim_options={'name': 'cosine', 'user_based': False})\n",
    "\n",
    "for trainset, testset in kf.split(movielens_dataset):\n",
    "    print(\"Fold number\", fold_nr)\n",
    "    item_algo.fit(trainset)\n",
    "    predictions = item_algo.test(testset)\n",
    "    pres_5, recs_5 = precision_recall_at_k(predictions, trainset.ur, k=5)\n",
    "    pres_10, recs_10 = precision_recall_at_k(predictions, trainset.ur, k=10)\n",
    "    \n",
    "    # Precision and recall averaged over all users for this fold\n",
    "    pre_5 = np.array([pres_5[k] for k in pres_5.keys()]).mean()\n",
    "    rec_5 = np.array([recs_5[k] for k in recs_5.keys()]).mean()\n",
    "    pre_10 = np.array([pres_10[k] for k in pres_10.keys()]).mean()\n",
    "    rec_10 = np.array([recs_10[k] for k in recs_10.keys()]).mean()\n",
    "\n",
    "    print(\"Precision and Recall @5:\", pre_5, rec_5)\n",
    "    print(\"Precision and Recall @10:\", pre_10, rec_10)\n",
    "    fold_nr += 1\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
